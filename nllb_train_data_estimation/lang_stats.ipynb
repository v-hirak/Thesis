{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLB-200 Training Data Statistics Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no official numbers about the amount of data for each language which was used to train the NLLB-200 model. Therefore, here I attempt to estimate the distribution over the languages.\n",
    "\n",
    "Training data consists of the following portions:\n",
    "\n",
    "- Primary bitext\n",
    "    - Public data\n",
    "    - Seed data\n",
    "- Mined bitext\n",
    "- Backtranslated bitext\n",
    "\n",
    "For more information on NLLB-200 training data, see https://github.com/facebookresearch/fairseq/tree/nllb/examples/nllb/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Bitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBL_PATH = \"/Users/hirak/nllb_train_data/primary/public_data\"\n",
    "\n",
    "EXCLUDED_FILES = [\"cached_lm_test.en\", \"test.fm.prob\", \"get_zero_shot_pairs.py\",\n",
    "                  \"zeroshotcorpstats\", \"README\", \"train.tsv\", \".DS_Store\"]\n",
    "\n",
    "# Mapping of ISO 639-1 codes to ISO 639-3 codes\n",
    "ISO_MAP = {\"bn\": \"ben\", \"en\": \"eng\", \"gu\": \"guj\", \"hi\": \"hin\", \"kn\": \"kan\",\n",
    "           \"ml\": \"mal\", \"mr\": \"mar\", \"or\": \"ory\", \"pa\": \"pan\", \"ta\": \"tam\",\n",
    "           \"te\": \"tel\", \"ur\": \"urd\", \"fr\": \"fra\"}\n",
    "\n",
    "# MISC_MAP = {\"tir_ET\": \"tir\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hirak/nllb_train_data/primary/public_data/aau\n",
      "/Users/hirak/nllb_train_data/primary/public_data/akuapem\n",
      "/Users/hirak/nllb_train_data/primary/public_data/bianet\n",
      "/Users/hirak/nllb_train_data/primary/public_data/cmu_hatian\n",
      "/Users/hirak/nllb_train_data/primary/public_data/ffr\n",
      "/Users/hirak/nllb_train_data/primary/public_data/french_ewe\n",
      "/Users/hirak/nllb_train_data/primary/public_data/french_fongbe\n",
      "/Users/hirak/nllb_train_data/primary/public_data/giossa\n",
      "/Users/hirak/nllb_train_data/primary/public_data/hornmt\n",
      "/Users/hirak/nllb_train_data/primary/public_data/indic_nlp\n",
      "/Users/hirak/nllb_train_data/primary/public_data/kinya_smt\n",
      "/Users/hirak/nllb_train_data/primary/public_data/lingala_songs\n",
      "/Users/hirak/nllb_train_data/primary/public_data/mburisano\n",
      "/Users/hirak/nllb_train_data/primary/public_data/menyo20k\n",
      "/Users/hirak/nllb_train_data/primary/public_data/minangnlp\n",
      "/Users/hirak/nllb_train_data/primary/public_data/mukiibi\n",
      "/Users/hirak/nllb_train_data/primary/public_data/nynorsk_memories\n",
      "/Users/hirak/nllb_train_data/primary/public_data/tico\n",
      "/Users/hirak/nllb_train_data/primary/public_data/til\n",
      "/Users/hirak/nllb_train_data/primary/public_data/umsuka\n",
      "/Users/hirak/nllb_train_data/primary/public_data/xhosa_navy\n"
     ]
    }
   ],
   "source": [
    "datasets_publ = sorted(os.listdir(PUBL_PATH))\n",
    "datasets_publ.remove(\".DS_Store\")\n",
    "\n",
    "file_paths_publ = []\n",
    "\n",
    "for dataset in datasets_publ:\n",
    "    dataset_path = os.path.join(PUBL_PATH, dataset)\n",
    "    dataset_langs = set()\n",
    "    for parent_dir, _, dir_files in os.walk(dataset_path):\n",
    "        dir_files = [file for file in dir_files if file not in EXCLUDED_FILES]\n",
    "        for file in dir_files:\n",
    "            file_lang = file.split(\".\")[1]\n",
    "            file_lang = ISO_MAP.get(file_lang, file_lang)\n",
    "            if not file_lang in dataset_langs:  # Avoid duplicate data\n",
    "                dataset_langs.add(file_lang)\n",
    "                file_path = os.path.join(parent_dir, file)\n",
    "                file_paths_publ.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_publ = []\n",
    "\n",
    "for parent_dir, _, dir_file_names in os.walk(PUBL_PATH):\n",
    "    dir_file_names = [n for n in dir_file_names if n not in EXCLUDED_FILES]\n",
    "    dir_file_paths = [parent_dir + \"/\" + name for name in dir_file_names]\n",
    "    file_paths_publ.extend(dir_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_sents_publ = defaultdict(set)\n",
    "\n",
    "for path in file_paths_publ:\n",
    "    lang = path.split(\".\")[-1]\n",
    "    lang = ISO_MAP.get(lang, lang)\n",
    "    with open(path) as file:\n",
    "        sents = file.readlines()\n",
    "        lang_sents_publ[lang].update(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_sizes_publ = {lang: len(set(sents)) for lang, sents in lang_sents_publ.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lang_sizes_publ.items(), columns=[\"lang\", \"num_sents_publ\"])\n",
    "df.sort_values(\"lang\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lang_train_size.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lang_train_size.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_PATH = \"/Users/hirak/nllb_train_data/primary/seed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_seed = []\n",
    "\n",
    "for parent_dir, _, dir_file_names in os.walk(SEED_PATH):\n",
    "    dir_file_paths = [parent_dir + \"/\" + name for name in dir_file_names]\n",
    "    file_paths_seed.extend(dir_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_sizes_seed = defaultdict(int)\n",
    "\n",
    "for path in file_paths_seed:\n",
    "    lang = path.split(\"/\")[-1]\n",
    "    with open(path) as file:\n",
    "        num_sents = len(file.readlines())\n",
    "        lang_sizes_seed[lang] += num_sents\n",
    "\n",
    "lang_sizes_seed[\"eng_Latn\"] = 6193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in lang_sizes_seed:\n",
    "    lang_sizes_seed[lang] = 6193 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'eng_Latn': 6193,\n",
       "             'tzm_Tfng': 6193,\n",
       "             'fur_Latn': 6193,\n",
       "             'ltg_Latn': 6193,\n",
       "             'mag_Deva': 6193,\n",
       "             'lij_Latn': 6193,\n",
       "             'pbt_Arab': 6193,\n",
       "             'knc_Latn': 6193,\n",
       "             'taq_Latn': 6193,\n",
       "             'srd_Latn': 6193,\n",
       "             'lim_Latn': 6193,\n",
       "             'dzo_Tibt': 6193,\n",
       "             'bho_Deva': 6193,\n",
       "             'mri_Latn': 6193,\n",
       "             'kas_Deva': 6193,\n",
       "             'bug_Latn': 6193,\n",
       "             'bjn_Arab': 6193,\n",
       "             'vec_Latn': 6193,\n",
       "             'dik_Latn': 6193,\n",
       "             'ace_Arab': 6193,\n",
       "             'grn_Latn': 6193,\n",
       "             'nus_Latn': 6193,\n",
       "             'szl_Latn': 6193,\n",
       "             'ary_Arab': 6193,\n",
       "             'ace_Latn': 6193,\n",
       "             'scn_Latn': 6193,\n",
       "             'bjn_Latn': 6193,\n",
       "             'arz_Arab': 6193,\n",
       "             'kas_Arab': 6193,\n",
       "             'hne_Deva': 6193,\n",
       "             'mni_Beng': 6193,\n",
       "             'knc_Arab': 6193,\n",
       "             'lmo_Latn': 6193,\n",
       "             'crh_Latn': 6193,\n",
       "             'fuv_Latn': 6193,\n",
       "             'taq_Tfng': 6193,\n",
       "             'prs_Arab': 6193,\n",
       "             'bam_Latn': 6193,\n",
       "             'ban_Latn': 6193,\n",
       "             'shn_Mymr': 6193})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_sizes_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"num_sents_seed\"] = df[\"lang\"].map(lang_sizes_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lang_train_size.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GERL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERL_PATH = \"/Users/hirak/nllb_train_data/primary/gerl.json\"\n",
    "gerl = json.load(open(GERL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gerl[2][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sents_gerl = 0\n",
    "\n",
    "for pair in data:\n",
    "    if pair[\"ee_sentence\"] and pair[\"eng_sentence\"]:\n",
    "        num_sents_gerl += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sents_gerl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mined Bitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Metadata URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "README_PATH = \"/Users/hirak/nllb_train_data/mined/README.md\"\n",
    "URL_PATTERN = r\"https://dl.fbaipublicfiles.com/nllb/data/\\w{3}_\\w{4}-\\w{3}_\\w{4}.meta.v1.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(README_PATH) as f:\n",
    "    readme = f.read()\n",
    "\n",
    "urls = re.findall(URL_PATTERN, readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metadata_urls.txt\", \"w\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Metadata from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metadata_urls.txt\") as f:\n",
    "    urls = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = urls[:3]\n",
    "\n",
    "for url in test_urls:\n",
    "    urlretrieve(url, filename=url.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unxz: code/ace_Latn-ban_Latn.meta.v1.xz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! unxz code/ace_Latn-ban_Latn.meta.v1.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mined = pd.read_csv(\"../ace_Latn-ban_Latn.meta.v1\", delimiter=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
